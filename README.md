#  Pipeline ETL Automatis√© de Donn√©es M√©t√©o (AWS & Apache Airflow)

##  Contexte du Projet
Dans le cadre de mon d√©veloppement en pipeline c√¥t√© prodution, je construis un pipeline ETL (Extract, Transform, Load) automatis√© de bout en bout. 
Ce projet extrait des donn√©es m√©t√©orologiques en temps r√©el pour une ville sp√©cifique depuis l'**API OpenWeatherMap**, applique des transformations avec **Python (Pandas)**, et charge automatiquement les donn√©es trait√©es sous forme de fichier CSV dans un Data Lake **Amazon S3**. L'ensemble du workflow est orchestr√© et planifi√© quotidiennement √† l'aide d'**Apache Airflow**, h√©berg√© sur une instance AWS EC2.

---

##  Stack Technique & Architecture
*   **Infrastructure Cloud :** AWS EC2 (Ubuntu, t2.small), AWS S3, AWS IAM Roles, AWS CLI (Jetons STS)
*   **Orchestration :** Apache Airflow (Standalone)
*   **Langage & Traitement des donn√©es :** Python 3.10, Pandas
*   **Connecteurs :** S3FS, API REST

---

##  Workflow du DAG Airflow
Le pipeline est d√©fini dans un Graphe Orient√© Acyclique (DAG) nomm√© `weather_etl_pipeline` et se compose de 3 t√¢ches s√©quentielles utilisant des op√©rateurs natifs d'Airflow :

1.  **`is_weather_api_ready` (HttpSensor) :** Interroge intelligemment le point de terminaison de l'API pour v√©rifier sa disponibilit√© et la validit√© des identifiants avant d'autoriser le pipeline √† se poursuivre.
2.  **`extract_weather_data` (SimpleHttpOperator) :** Ex√©cute une requ√™te GET vers l'API OpenWeatherMap, filtre la r√©ponse et la convertit au format JSON.
3.  **`transform_load_weather_data` (PythonOperator) :** 
    *   R√©cup√®re les donn√©es JSON brutes de la t√¢che pr√©c√©dente en utilisant le syst√®me de messagerie interne **Airflow XComs**.
    *   Transforme les donn√©es (ex : conversion des temp√©ratures de Kelvin vers Fahrenheit, formatage des horodatages) avec **Pandas**.
    *   Charge de mani√®re s√©curis√©e le DataFrame final structur√© directement dans un bucket Amazon S3 sous la forme d'un fichier CSV horodat√©.

---

## üìÅ Structure du D√©p√¥t
```text
automated-weather-etl-airflow/
‚îÇ
‚îú‚îÄ‚îÄ dags/                       
‚îÇ   ‚îî‚îÄ‚îÄ weather_dag.py          # La d√©finition du DAG Airflow et les fonctions Python
‚îÇ
‚îú‚îÄ‚îÄ images/                     # Preuves d'ex√©cution et captures d'√©cran de l'UI
‚îÇ   ‚îú‚îÄ‚îÄ airflow_login.png       
‚îÇ   ‚îú‚îÄ‚îÄ airflow_dags.png        
‚îÇ   ‚îî‚îÄ‚îÄ aws_s3_success.png      
‚îÇ
‚îú‚îÄ‚îÄ .gitignore                  
‚îú‚îÄ‚îÄ requirements.txt            # D√©pendances Python (pandas, s3fs, apache-airflow)
‚îî‚îÄ‚îÄ README.md                   
```
---

## Configuration de l'Infrastructure
- L'environnement a √©t√© configur√© sur une instance AWS EC2 en utilisant un environnement virtuel Python (airflow_venv). Airflow fonctionne sur le port 8080 (R√®gle de s√©curit√© Custom TCP).
Vous pouvez visualisez sur image quelques √©tapes de mon projet en capture d'√©cran au fur et √† mesure de mon avancement.
---

## Bonnes Pratiques de S√©curit√© Impl√©ment√©es
‚Ä¢ Aucun secret en dur : Les cl√©s API sont g√©r√©es de mani√®re s√©curis√©e via l'interface des Connexions internes d'Airflow (weather_map_api).
‚Ä¢ R√¥les IAM & AWS CLI : Les identifiants AWS (Cl√©s d'acc√®s & Jetons de session) ont √©t√© configur√©s via AWS CLI pour autoriser de fa√ßon s√©curis√©e l'instance EC2 √† √©crire dans le bucket S3 sans exposer de cl√©s dans le code source.
##  Prochaines √âtapes & Am√©liorations
‚Ä¢ Finaliser le projet.
‚Ä¢ Impl√©menter un pipeline CI/CD avec GitHub Actions pour automatiser le d√©ploiement des DAGs sur le serveur EC2.
‚Ä¢ Conteneuriser l'environnement Airflow en utilisant Docker & Docker Compose.
